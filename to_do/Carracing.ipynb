{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Carracing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQvyJYa54Apn",
        "colab_type": "text"
      },
      "source": [
        "# Install dependancies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njNnHn1w4DzZ",
        "colab_type": "text"
      },
      "source": [
        "Atari dependancies - for pacman testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXA5zfH04Gqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools > /dev/null 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TsytGtU4ft_",
        "colab_type": "text"
      },
      "source": [
        "Rendering dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlTI1kaO3Zpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-rxWwlm6B7Q",
        "colab_type": "text"
      },
      "source": [
        "Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u8_Xxgc6Ac7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev > /dev/null 2>&1\n",
        "!pip install stable-baselines==2.4.0 box2d box2d-kengz > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbNgacxn4KLm",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVuk1ksv4Gws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "import os \n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vGpWvbT4MOt",
        "colab_type": "code",
        "outputId": "0ac2951e-fcda-447a-d4c2-e6f09ae256e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 12:33:57.974799 139753948342144 abstractdisplay.py:144] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x9SouKd6W9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.common.policies import CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy, MlpPolicy\n",
        "# from stable_baselines.ddpg.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.ppo2 import PPO2\n",
        "# from stable_baselines.ddpg import DDPG\n",
        "# from stable_baselines.deepq import DQN, MlpPolicy\n",
        "# from stable_baselines.ddpg.noise import AdaptiveParamNoiseSpec\n",
        "\n",
        "# from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcrRd4o9qGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"IMPORT PACKAGES\"\"\"\n",
        "\n",
        "from signal import signal, SIGPIPE, SIG_DFL\n",
        "signal(SIGPIPE, SIG_DFL) \n",
        "\n",
        "import argparse\n",
        "\n",
        "'''Model selection '''\n",
        "from stable_baselines import logger\n",
        "\n",
        "# from stable_baselines.ppo2 import PPO2\n",
        "# from stable_baselines.common.policies import CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy, MlpPolicy\n",
        "\n",
        "'''Vectorized Env'''\n",
        "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv, VecFrameStack\n",
        "from stable_baselines.common.cmd_util import make_atari_env\n",
        "from stable_baselines.common import set_global_seeds\n",
        "\n",
        "'''Monitoring Learning process'''\n",
        "# from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "from stable_baselines.common.atari_wrappers import MaxAndSkipEnv, FrameStack\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0xCv6_OTzpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools as it\n",
        "from skimage import color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUbAfmu94P42",
        "colab_type": "text"
      },
      "source": [
        "# Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YGaOb_j6gOt",
        "colab_type": "text"
      },
      "source": [
        "define plotting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axga0cul6frp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Plotting'''\n",
        "\n",
        "def movingAverage(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve', smoothing = True):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "\n",
        "    if smoothing:\n",
        "        y = movingAverage(y, window=50)\n",
        "    else:\n",
        "        title = 'Learning Curve no smoothing'\n",
        "\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aHquJib6lXn",
        "colab_type": "text"
      },
      "source": [
        "define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5SO7yX6k7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, env, num_steps=1000):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_steps: (int) number of timesteps to evaluate it\n",
        "    :return: (float) Mean reward for the last 100 episodes\n",
        "    \"\"\"\n",
        "    episode_rewards = [0.0]\n",
        "    obs = env.reset()\n",
        "    for i in range(num_steps):\n",
        "        # _states are only useful when using LSTM policies\n",
        "        action, _states = model.predict(obs)\n",
        "        # here, action, rewards and dones are arrays\n",
        "        # because we are using vectorized env\n",
        "        obs, rewards, dones, info = env.step(action)\n",
        "\n",
        "        # Stats\n",
        "        episode_rewards[-1] += rewards[0]\n",
        "        if dones[0]:\n",
        "            obs = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "    # Compute mean reward for the last 100 episodes\n",
        "    mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "    print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "\n",
        "    return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzD0y67W-U3d",
        "colab_type": "text"
      },
      "source": [
        "define callback function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhiQobYb-UVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def callback(_locals, _globals):\n",
        "  \"\"\"\n",
        "  Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "  :param _locals: (dict)\n",
        "  :param _globals: (dict)\n",
        "  \"\"\"\n",
        "  global n_steps, best_mean_reward\n",
        "  # Print stats every 1000 calls\n",
        "  if (n_steps + 1) % 100000 == 0:\n",
        "      # Evaluate policy performance\n",
        "      x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "      if len(x) > 0:\n",
        "          mean_reward = np.mean(y[-100:])\n",
        "          print(x[-1], 'timesteps')\n",
        "          print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
        "\n",
        "          # New best model, you could save the agent here\n",
        "          if mean_reward > best_mean_reward:\n",
        "              best_mean_reward = mean_reward\n",
        "              # Example for saving best model\n",
        "              print(\"Saving new best model\")\n",
        "              _locals['self'].save(log_dir + 'best_model.pkl')\n",
        "  n_steps += 1\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4jHScupD3BN",
        "colab_type": "text"
      },
      "source": [
        "define video function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjhCjmI4Un9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "#   env = Monitor(env, './video', allow_early_resets=True)\n",
        "#   env = Monitor(env, './video')\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6yXbofrSuah",
        "colab_type": "text"
      },
      "source": [
        "# DQN agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ-R7o2uOeLl",
        "colab_type": "text"
      },
      "source": [
        "# Tweak Environment\n",
        "from source 'CarRacing - master'\n",
        "link will be provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM7MDMaB9-aJ",
        "colab_type": "code",
        "outputId": "ffbea59c-b110-42a1-b53b-4fafbece4703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "all_actions = np.array([k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])\n",
        "print(all_actions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.   1.   0.2]\n",
            " [-1.   1.   0. ]\n",
            " [-1.   0.   0.2]\n",
            " [-1.   0.   0. ]\n",
            " [ 0.   1.   0.2]\n",
            " [ 0.   1.   0. ]\n",
            " [ 0.   0.   0.2]\n",
            " [ 0.   0.   0. ]\n",
            " [ 1.   1.   0.2]\n",
            " [ 1.   1.   0. ]\n",
            " [ 1.   0.   0.2]\n",
            " [ 1.   0.   0. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBlwbEqrOmPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CarRacing(PPO2):\n",
        "    \"\"\"\n",
        "    CarRacing specifig part of the DQN-agent\n",
        "\n",
        "    Some minor env-specifig tweaks but overall\n",
        "    assumes very little knowledge from the environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_negative_rewards=100):\n",
        "        all_actions = np.array(\n",
        "            [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])]\n",
        "        )\n",
        "        # car racing env gives wrong pictures without render\n",
        "        kwargs[\"render\"] = True\n",
        "        super().__init__(\n",
        "            action_map=all_actions,\n",
        "            pic_size=(96, 96)\n",
        "        )\n",
        "\n",
        "        self.gas_actions = np.array([a[1] == 1 and a[2] == 0 for a in all_actions])\n",
        "        self.break_actions = np.array([a[2] == 1 for a in all_actions])\n",
        "        self.n_gas_actions = self.gas_actions.sum()\n",
        "        self.neg_reward_counter = 0\n",
        "        self.max_neg_rewards = max_negative_rewards\n",
        "\n",
        "    @staticmethod\n",
        "    def process_image(obs):\n",
        "        return 2 * color.rgb2gray(obs) - 1.0\n",
        "\n",
        "    def get_random_action(self):\n",
        "        \"\"\"\n",
        "        Here random actions prefer gas to break\n",
        "        otherwise the car can never go anywhere.\n",
        "        \"\"\"\n",
        "        action_weights = 14.0 * self.gas_actions + 1.0\n",
        "        action_weights /= np.sum(action_weights)\n",
        "\n",
        "        return np.random.choice(self.dim_actions, p=action_weights)\n",
        "\n",
        "    def check_early_stop(self, reward, totalreward):\n",
        "        if reward < 0:\n",
        "            self.neg_reward_counter += 1\n",
        "            done = (self.neg_reward_counter > self.max_neg_rewards)\n",
        "\n",
        "            if done and totalreward <= 500:\n",
        "                punishment = -20.0\n",
        "            else:\n",
        "                punishment = 0.0\n",
        "            if done:\n",
        "                self.neg_reward_counter = 0\n",
        "\n",
        "            return done, punishment\n",
        "        else:\n",
        "            self.neg_reward_counter = 0\n",
        "            return False, 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gp2PO-48N2j",
        "colab_type": "text"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IExUR-xj4X_s",
        "colab_type": "text"
      },
      "source": [
        "Building Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC0pgBul8R6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Train and save the DQN model, for the cartpole problem\n",
        "\n",
        "    :param args: (ArgumentParser) the input arguments\n",
        "    \"\"\"\n",
        "    print(\"Making a new model\")\n",
        "\n",
        "\n",
        "    env = wrap_env(gym.make('CarRacing-v0'))\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "\n",
        "    ''' For framestack(lstm)   '''\n",
        "    # env = FrameStack(env, 4)\n",
        "    # env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "#     env = VecFrameStack(make_atari_env(env_id=env, num_env=, seed=777), 4)\n",
        "    model = CarRacing(policy=CnnPolicy, env=env, n_steps=512, nminibatches=256,\n",
        "                 lam=0.95, gamma=0.99, noptepochs=8, ent_coef=0.01,\n",
        "                 learning_rate=5e-4, cliprange= lambda f : f * 0.2, verbose=0)\n",
        "\n",
        "    # mean_reward_before_train = evaluate(model, env=env, num_steps=10000)\n",
        "    print(\"Learning started. It takes some time...\")\n",
        "    model.learn(total_timesteps=1000, callback=callback)\n",
        "    # mean_reward = evaluate(model, env=env, num_steps=10000)\n",
        "    print(\"Saving model to CarRacing_model.pkl\")\n",
        "    model.save(\"CarRacing_model_PPO2_5\")\n",
        "    print(\"Plotting Learning Curve\")\n",
        "    plot_results(log_dir)\n",
        "    plot_results(log_dir, smoothing=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_gQsTci9Akf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run():\n",
        "    \"\"\"\n",
        "    Run a trained model for the pong problem\n",
        "    \"\"\"\n",
        "    env = wrap_env(gym.make('CarRacing-v0'))\n",
        "    print(\"wrapping complete\")\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "\n",
        "#     model = PPO2.load(\"CarRacing_model_PPO1_\"+ str(5) +\".pkl\", env)\n",
        "#     avg_rew = evaluate(model=model, env=env, num_steps=10000)\n",
        "#     model = PPO2.load(\"CarRacing_model_PPO2_5.pkl\", env)\n",
        "    model = CarRacing.load(\"CarRacing_model_PPO2_5.pkl\", env)\n",
        "    while True:\n",
        "        obs, done = env.reset(), False\n",
        "        episode_rew = 0\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action, _ = model.predict(obs)\n",
        "            obs, rew, done, _ = env.step(action)\n",
        "\n",
        "            episode_rew += rew\n",
        "        if done: \n",
        "          break;\n",
        "        print(\"Episode reward\", episode_rew)\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV0lq8ha8_R4",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxp7kfVO9J0K",
        "colab_type": "code",
        "outputId": "69aaae40-c32f-4fc0-824d-18dd4d343788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # Create log dir\n",
        "    log_dir = \"data/\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    best_mean_reward, n_steps = -np.inf, 0\n",
        "\n",
        "#     run()\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making a new model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1fbdb684e204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-47ab4d339f7e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     model = CarRacing(policy=CnnPolicy, env=env, n_steps=512, nminibatches=256,\n\u001b[1;32m     19\u001b[0m                  \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoptepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                  learning_rate=5e-4, cliprange= lambda f : f * 0.2, verbose=0)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# mean_reward_before_train = evaluate(model, env=env, num_steps=10000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3ab4817495de>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_negative_rewards, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpic_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'action_map'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOU3f2ACAySt",
        "colab_type": "text"
      },
      "source": [
        "# Others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5YV1Bub8U0j",
        "colab_type": "text"
      },
      "source": [
        "Pacman - for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVH87BSY4VHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "# #check out the pacman action space!\n",
        "# print(env.action_space)\n",
        "# observation = env.reset()\n",
        "\n",
        "# while True:\n",
        "  \n",
        "#     env.render()\n",
        "    \n",
        "#     #your agent goes here\n",
        "#     action = env.action_space.sample() \n",
        "         \n",
        "#     observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "#     if done: \n",
        "#       break;\n",
        "            \n",
        "# env.close()\n",
        "# show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}