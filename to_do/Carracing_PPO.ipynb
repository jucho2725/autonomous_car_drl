{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Carracing_PPO.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQvyJYa54Apn",
        "colab_type": "text"
      },
      "source": [
        "# Install dependancies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njNnHn1w4DzZ",
        "colab_type": "text"
      },
      "source": [
        "Atari dependancies - for pacman testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXA5zfH04Gqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools > /dev/null 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TsytGtU4ft_",
        "colab_type": "text"
      },
      "source": [
        "Rendering dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlTI1kaO3Zpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-rxWwlm6B7Q",
        "colab_type": "text"
      },
      "source": [
        "Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u8_Xxgc6Ac7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev > /dev/null 2>&1\n",
        "!pip install stable-baselines==2.6.0 box2d box2d-kengz > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbNgacxn4KLm",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVuk1ksv4Gws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor as gymMonitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "import os \n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vGpWvbT4MOt",
        "colab_type": "code",
        "outputId": "46c5ed1f-6ddf-4975-d5ee-3ca09babbac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 13:27:33.400539 140665384146816 abstractdisplay.py:144] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x9SouKd6W9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.common.policies import CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy, MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.ppo2 import PPO2\n",
        "\n",
        "\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcrRd4o9qGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"IMPORT PACKAGES\"\"\"\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "'''Model selection '''\n",
        "from stable_baselines import logger\n",
        "\n",
        "\n",
        "'''Vectorized Env'''\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "\n",
        "'''Monitoring Learning process'''\n",
        "from stable_baselines.bench import Monitor \n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "from stable_baselines.common.atari_wrappers import MaxAndSkipEnv, FrameStack\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0xCv6_OTzpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools as it\n",
        "from skimage import color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUbAfmu94P42",
        "colab_type": "text"
      },
      "source": [
        "# Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YGaOb_j6gOt",
        "colab_type": "text"
      },
      "source": [
        "define plotting function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aHquJib6lXn",
        "colab_type": "text"
      },
      "source": [
        "define evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5SO7yX6k7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, env, num_steps=1000):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_steps: (int) number of timesteps to evaluate it\n",
        "    :return: (float) Mean reward for the last 100 episodes\n",
        "    \"\"\"\n",
        "    episode_rewards = [0.0]\n",
        "    obs = env.reset()\n",
        "    for i in range(num_steps):\n",
        "        # _states are only useful when using LSTM policies\n",
        "        action, _states = model.predict(obs)\n",
        "        # here, action, rewards and dones are arrays\n",
        "        # because we are using vectorized env\n",
        "        obs, rewards, dones, info = env.step(action)\n",
        "\n",
        "        # Stats\n",
        "        episode_rewards[-1] += rewards[0]\n",
        "        if dones[0]:\n",
        "            obs = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "    # Compute mean reward for the last 100 episodes\n",
        "    mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "    print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "\n",
        "    return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzD0y67W-U3d",
        "colab_type": "text"
      },
      "source": [
        "define callback function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhiQobYb-UVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\"\n",
        "    global n_steps, best_mean_reward\n",
        "    # Print stats every 1000 calls\n",
        "    if (n_steps + 1) % 100000 == 0:\n",
        "        # Evaluate policy performance\n",
        "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "        if len(x) > 0:\n",
        "            mean_reward = np.mean(y[-100:])\n",
        "            print(x[-1], 'timesteps')\n",
        "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
        "\n",
        "            # New best model, you could save the agent here\n",
        "            if mean_reward > best_mean_reward:\n",
        "                best_mean_reward = mean_reward\n",
        "                # Example for saving best model\n",
        "                print(\"Saving new best model\")\n",
        "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
        "    n_steps += 1\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4jHScupD3BN",
        "colab_type": "text"
      },
      "source": [
        "define video function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjhCjmI4Un9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        p4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = gymMonitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ-R7o2uOeLl",
        "colab_type": "text"
      },
      "source": [
        "# Tweak Environment(Discrete)\n",
        "from source 'CarRacing - master'\n",
        "\n",
        "link will be provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM7MDMaB9-aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all_actions = np.array([k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])])\n",
        "# print(all_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBlwbEqrOmPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class CarRacing(PPO2):\n",
        "#     \"\"\"\n",
        "#     CarRacing specifig part of the DQN-agent\n",
        "\n",
        "#     Some minor env-specifig tweaks but overall\n",
        "#     assumes very little knowledge from the environment\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, max_negative_rewards=100):\n",
        "#         all_actions = np.array(\n",
        "#             [k for k in it.product([-1, 0, 1], [1, 0], [0.2, 0])]\n",
        "#         )\n",
        "#         # car racing env gives wrong pictures without render\n",
        "#         kwargs[\"render\"] = True\n",
        "#         super().__init__(\n",
        "#             action_map=all_actions,\n",
        "#             pic_size=(96, 96)\n",
        "#         )\n",
        "\n",
        "#         self.gas_actions = np.array([a[1] == 1 and a[2] == 0 for a in all_actions])\n",
        "#         self.break_actions = np.array([a[2] == 1 for a in all_actions])\n",
        "#         self.n_gas_actions = self.gas_actions.sum()\n",
        "#         self.neg_reward_counter = 0\n",
        "#         self.max_neg_rewards = max_negative_rewards\n",
        "\n",
        "#     @staticmethod\n",
        "#     def process_image(obs):\n",
        "#         return 2 * color.rgb2gray(obs) - 1.0\n",
        "\n",
        "#     def get_random_action(self):\n",
        "#         \"\"\"\n",
        "#         Here random actions prefer gas to break\n",
        "#         otherwise the car can never go anywhere.\n",
        "#         \"\"\"\n",
        "#         action_weights = 14.0 * self.gas_actions + 1.0\n",
        "#         action_weights /= np.sum(action_weights)\n",
        "\n",
        "#         return np.random.choice(self.dim_actions, p=action_weights)\n",
        "\n",
        "#     def check_early_stop(self, reward, totalreward):\n",
        "#         if reward < 0:\n",
        "#             self.neg_reward_counter += 1\n",
        "#             done = (self.neg_reward_counter > self.max_neg_rewards)\n",
        "\n",
        "#             if done and totalreward <= 500:\n",
        "#                 punishment = -20.0\n",
        "#             else:\n",
        "#                 punishment = 0.0\n",
        "#             if done:\n",
        "#                 self.neg_reward_counter = 0\n",
        "\n",
        "#             return done, punishment\n",
        "#         else:\n",
        "#             self.neg_reward_counter = 0\n",
        "#             return False, 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT-nfg_88tzm",
        "colab_type": "text"
      },
      "source": [
        "# Tweak Enviornments(Continuous)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxJZ2jjw_rVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym import Wrapper\n",
        "from gym import spaces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xapt11U_8tML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "low = np.array([-1.0, 0, 0])\n",
        "high = np.array([1.0, 1.0, 0.2])\n",
        "\n",
        "class ControlCarRacing(Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ControlCarRacing, self).__init__(env)\n",
        "        self.action_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
        "\n",
        "        \n",
        "# To do : Tweak Model - model action 함수가 continuous distrbution 안에 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gp2PO-48N2j",
        "colab_type": "text"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IExUR-xj4X_s",
        "colab_type": "text"
      },
      "source": [
        "Building Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC0pgBul8R6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Train and save the DQN model, for the cartpole problem\n",
        "\n",
        "    :param args: (ArgumentParser) the input arguments\n",
        "    \"\"\"\n",
        "    print(\"Making a new model\")\n",
        "\n",
        "\n",
        "    env = wrap_env(gym.make('CarRacing-v0'))\n",
        "    env = ControlCarRacing(env)\n",
        "#     env = MaxAndSkipEnv(env, skip=4)\n",
        "\n",
        "    ''' For framestack(lstm)   '''\n",
        "#     env = FrameStack(env, 4)\n",
        "    env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "    model = PPO2(policy=CnnPolicy, env=env, n_steps=128, nminibatches=4,\n",
        "                 noptepochs=10, learning_rate=3e-4, cliprange= lambda f : f * 0.2, verbose=0, tensorboard_log='graph/')\n",
        "\n",
        "    print(\"Learning started. It takes some time...\")\n",
        "    model.learn(total_timesteps=300000, callback=callback)\n",
        "    print(\"Saving model to CarRacing_model.pkl\")\n",
        "    model.save(\"ppo2_nofr_noskip.pkl\")\n",
        "    print(\"Plotting Learning Curve\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV0lq8ha8_R4",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxp7kfVO9J0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # Create log dir\n",
        "    log_dir = \"data/\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    best_mean_reward, n_steps = -np.inf, 0\n",
        "\n",
        "#     run()\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px9_edYGH7A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PRdqzw9dNWH",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHTCfhBSdOw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# log_dir = \"/tmp/gym/\"\n",
        "# os.makedirs(log_dir, exist_ok=True)\\\n",
        "\n",
        "# env = wrap_env(gym.make('CarRacing-v0'))\n",
        "# env = DiscreteCarRacing(env)\n",
        "# # env = MaxAndSkipEnv(env, skip=4)\n",
        "# env = FrameStack(env, 4)\n",
        "# env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxyThlFHdPbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def evaluate(model, env, num_steps=1000):\n",
        "#     \"\"\"\n",
        "#     Evaluate a RL agent\n",
        "#     :param model: (BaseRLModel object) the RL Agent\n",
        "#     :param num_steps: (int) number of timesteps to evaluate it\n",
        "#     :return: (float) Mean reward for the last 100 episodes\n",
        "#     \"\"\"\n",
        "#     episode_rewards = [0.0]\n",
        "#     obs = env.reset()\n",
        "#     for i in range(num_steps):\n",
        "#         # _states are only useful when using LSTM policies\n",
        "#         action, _states = model.predict(obs)\n",
        "#         # here, action, rewards and dones are arrays\n",
        "#         # because we are using vectorized env\n",
        "#         obs, rewards, dones, info = env.step(action)\n",
        "\n",
        "#         # Stats\n",
        "#         episode_rewards[-1] += rewards[0]\n",
        "#         if dones[0]:\n",
        "#             obs = env.reset()\n",
        "#             episode_rewards.append(0.0)\n",
        "#     # Compute mean reward for the last 100 episodes\n",
        "#     mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "#     print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "\n",
        "#     return mean_100ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Oz-CwUdnPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvuNQYpBdSx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = PPO2.load(\"ppo2_noskip.pkl\")\n",
        "# mean_reward = evaluate(model, env=env, num_steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOU3f2ACAySt",
        "colab_type": "text"
      },
      "source": [
        "# Others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5YV1Bub8U0j",
        "colab_type": "text"
      },
      "source": [
        "Pacman - for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVH87BSY4VHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "# #check out the pacman action space!\n",
        "# print(env.action_space)\n",
        "# observation = env.reset()\n",
        "\n",
        "# while True:\n",
        "  \n",
        "#     env.render()\n",
        "    \n",
        "#     #your agent goes here\n",
        "#     action = env.action_space.sample() \n",
        "         \n",
        "#     observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "#     if done: \n",
        "#       break;\n",
        "            \n",
        "# env.close()\n",
        "# show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6inbtGwvccgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygXOHsZvxt-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}